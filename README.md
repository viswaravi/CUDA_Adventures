# CUDA Adventures

## Overview
This repository contains a series of progressively advancing CUDA programming tasks designed to strengthen my understanding of GPU programming and optimization focused on Computer Vision and LLMs. Work in progress.

## Sections
### **1. Vector Addition**
**Goal:** Refresh the CUDA fundamentals and implement basic vector addition kernels. Kernel profiling using NSight Compute and NSight Systems.

### **2. Reduction, Prefix Sum**
**Goal:** Deepen the understanding of memory access patterns and warp-level optimizations and shared memory usage for 1D data. Implement optimized version of kernels and compare performance gains through profiling.

### **3. Matrices - Multiplication, Rotation**
**Goal:** Deepen the understanding of memory access patterns and warp-level optimizations and shared memory usage for 2D data. Implement kernels for matrix multiplication, rotation.

### **4. Computer Vision - Convolution, Fast Convolution Kernels**
**Goal:** Optimize the image processing kernels for key operations in Convolutional Neural Networks (CNNs), Fast Convolution. 

<!-- Integration with CUDA Graphs and benchmark the performance difference. -->
<!-- ### **5. Fast Matrix Multiplication**
**Goal:** Optimize matrix multiplication using hardware-specific features. Utilize Tensor Cores. Test performance scaling.

### **6. Transformer Attention Optimization**
**Goal:** Develop and optimize kernels for softmax and self-attention mechanisms used in Transformer architectures.

### **7. Sparse Matrix Operations for LLM Efficiency**
**Goal:** Optimize sparse matrix operations, crucial for large language model (LLM) efficiency.

### **8. Model Quantization & TensorRT Optimization**
**Goal:** Optimize models for inference by applying quantization and leveraging TensorRT.

### **9. Optimizing GPT Model**
**Goal:** Apply all learned CUDA techniques to optimize a GPT-style model for inference and training efficiency. Transformer layers and custom KV-cache implementation.

### **10. Multi-GPU Training & NCCL Optimization (Project 8)**
**Goal:** Scale the  optimizations across multiple GPUs for training large models. -->